#!/usr/bin/env python
# -*- coding: utf-8 -*-

# # Meta Motivo Tutorial
# This notebook provides a simple introduction on how to use the Meta Motivo api.

# ## All imports
from packaging.version import Version
from metamotivo.fb_cpr.huggingface import FBcprModel
from huggingface_hub import hf_hub_download
from humenv import make_humenv
import gymnasium
from gymnasium.wrappers import FlattenObservation, TransformObservation, RecordEpisodeStatistics, RecordVideo
from metamotivo.buffers.buffers import DictBuffer
from humenv.env import make_from_name
from humenv import rewards as humenv_rewards

import torch
import mediapy as media
import math
import h5py
from pathlib import Path
import numpy as np
import os

unbalanced = True

# ## Model download
# The first step is to download the model. We show how to use HuggingFace hub for that.
model = FBcprModel.from_pretrained("facebook/metamotivo-S-1")
print(model)

# **Run a policy from Meta Motivo:**
# 
# Now that we saw how to load a pre-trained Meta Motivo policy, we can prompt it and execute actions with it. 
# 
# The first step is to sample a context embedding `z` that needs to be passed to the policy.
device = "cpu"

if Version("0.26") <= Version(gymnasium.__version__) < Version("1.0"):
    transform_obs_wrapper = lambda env: TransformObservation(
            env, lambda obs: torch.tensor(obs.reshape(1, -1), dtype=torch.float32, device=device)
        )
else:
    transform_obs_wrapper = lambda env: TransformObservation(
            env, lambda obs: torch.tensor(obs.reshape(1, -1), dtype=torch.float32, device=device), env.observation_space
        )

# Create video directory if it doesn't exist
video_dir = os.path.join(os.getcwd(), "videos")
os.makedirs(video_dir, exist_ok=True)

env, _ = make_humenv(
    num_envs=1,
    unbalanced=unbalanced,
    wrappers=[
        FlattenObservation,
        transform_obs_wrapper,
        RecordEpisodeStatistics,
        lambda env: RecordVideo(env, video_dir, episode_trigger=lambda x: True)  # Record every episode
    ],
    state_init="Default",
)

model.to(device)
z = model.sample_z(1)
print(f"embedding size {z.shape}")
print(f"z norm: {torch.norm(z)}")
print(f"z norm / sqrt(d): {torch.norm(z) / math.sqrt(z.shape[-1])}")
observation, _ = env.reset()

# Run the episode and let RecordVideo wrapper handle recording
for i in range(3000):
    action = model.act(observation, z, mean=True)
    observation, reward, terminated, truncated, info = env.step(action.cpu().numpy().ravel())
    if terminated or truncated:
        observation, _ = env.reset()

print(f"Video recording saved to {video_dir}")

# ### Computing Q-functions
# 
# FB-CPR provides a way of directly computing the action-value function of any policy embedding `z` on any task embedding `z_r`. Then, the Q function of a policy $z$ is given by
# 
# $Q(s,a, z) = F(s,a,z) \cdot z_r$
# 
# The task embedding can be computed in the following way. Given a set of samples labeled with rewards $(s,a,s',r)$, the task embedding is given by: 
# 
# $z_r = \mathrm{normalised}(\sum_{i \in \mathrm{batch}} r_i B(s'_i))$.
def Qfunction(state, action, z_reward, z_policy):
    F = model.forward_map(obs=state, z=z_policy.repeat(state.shape[0],1), action=action) # num_parallel x num_samples x z_dim
    Q = F @ z_reward.ravel()
    return Q.mean(axis=0)

z_reward = model.sample_z(1)
z_policy = model.sample_z(1)
state = torch.rand((10, env.observation_space.shape[0]), device=model.cfg.device, dtype=torch.float32)
action = torch.rand((10, env.action_space.shape[0]), device=model.cfg.device, dtype=torch.float32)*2 - 1
Q = Qfunction(state, action, z_reward, z_policy)
print(Q)

# ## Prompting the model
# 
# We have seen that we can condition the model via the context variable `z`. We can control the task to execute via _prompting_ (or _policy inference_).

# ### Reward prompts
# The first version of inference we investigate is the reward prompting, i.e., given a set of reward label samples we can infer in a zero-shot way the near-optimal policy for solving such task.
# 
# First step, download the data for inference. We provide a buffer for inference of about 500k samples. This buffer has been generated by randomly subsampling the final replay buffer.
local_dir = "metamotivo-S-1-datasets"
dataset = "buffer_inference_500000.hdf5"
buffer_path = hf_hub_download(
        repo_id="facebook/metamotivo-S-1",
        filename=f"data/{dataset}",
        repo_type="model",
        local_dir=local_dir,
    )
print(buffer_path)

# Now that we have download the h5 file for inference, we can conveniently loaded it in a buffer.
hf = h5py.File(buffer_path, "r")
print(hf.keys())
data = {}
for k, v in hf.items():
    print(f"{k:20s}: {v.shape}")
    data[k] = v[:]
buffer = DictBuffer(capacity=data["qpos"].shape[0], device="cpu")
buffer.extend(data)
del data

batch = buffer.sample(5)
for k, v in batch.items():
    print(f"{k:20s}: {v.shape}")

# As you can see, the buffer does not provide a reward signal. We need to label this buffer with the desired reward function. We provide API for that but here we start looking into the basic steps:
# * Instantiate a reward function
# * Computing the reward from the batch data
reward_fn = humenv_rewards.LocomotionReward(move_speed=2.0) # move ahead with speed 2
# humenv provides also a name-base reward initialization. We could
# get the same reward function in this way
reward_fn = make_from_name("move-ego-0-2") 
print(reward_fn)

# We can call the method `__call__` to obtain a reward value from the physics state. This function receives a mujoco model, qpos, qvel and the action. See the humenv tutorial for more information.
N = 100_000
batch = buffer.sample(N)
rewards = []
for i in range(N):
    rewards.append(
        reward_fn(
            env.unwrapped.model,
            qpos=batch["next_qpos"][i],
            qvel=batch["next_qvel"][i],
            ctrl=batch["action"][i])
    )
rewards = np.stack(rewards).reshape(-1,1)
print(rewards.ravel())

# **Note** that the reward functions implemented in humenv are functions of next state and action which means we need to use `next_qpos` and `next_qvel` that are the physical state of the system at the next state.
# 
# We provide a multi-thread version for faster relabeling, see `metamotivo.wrappers.humenvbench.relabel`.
from metamotivo.wrappers.humenvbench import relabel
rewards = relabel(
    env,
    qpos=batch["next_qpos"],
    qvel=batch["next_qvel"],
    action=batch["action"],
    reward_fn=reward_fn, 
    max_workers=8
)
print(rewards.ravel())

# We can now infer the context `z` for the selected task.
z = model.reward_wr_inference(
    next_obs=batch["next_observation"],
    reward=torch.tensor(rewards, device=model.cfg.device, dtype=torch.float32)
)
print(z.shape)

# Create a new environment with RecordVideo wrapper for reward prompt example
reward_video_dir = os.path.join(os.getcwd(), "videos_reward_prompt")
os.makedirs(reward_video_dir, exist_ok=True)

env_reward, _ = make_humenv(
    num_envs=1,
    unbalanced=unbalanced,
    wrappers=[
        FlattenObservation,
        transform_obs_wrapper,
        RecordEpisodeStatistics,
        lambda env: RecordVideo(env, reward_video_dir, episode_trigger=lambda x: True)
    ],
    state_init="Default",
)

observation, _ = env_reward.reset()
for i in range(3000):
    action = model.act(observation, z, mean=True)
    observation, reward, terminated, truncated, info = env_reward.step(action.cpu().numpy().ravel())
    if terminated or truncated:
        observation, _ = env_reward.reset()

print(f"Reward prompt video recording saved to {reward_video_dir}")

# Let's compute the **Q-function** for this policy.
z_reward = torch.sum(
    model.backward_map(obs=batch["next_observation"]) * torch.tensor(rewards, dtype=torch.float32, device=model.cfg.device),
    dim=0
)
z_reward = model.project_z(z_reward)
Q = Qfunction(batch["observation"], batch["action"], z_reward, z)
print(Q)

# # Goal and Tracking prompts
# The model supports two other modalities, `goal` and `tracking`. These two modalities expose similar functions for context inference:
# - `def goal_inference(self, next_obs: torch.Tensor) -> torch.Tensor`
# - `def tracking_inference(self, next_obs: torch.Tensor) -> torch.Tensor`
#   
# We show an example on how to perform goal inference.
goal_qpos = np.array([0.13769039,-0.20029453,0.42305034,0.21707786,0.94573617,0.23868944
,0.03856998,-1.05566834,-0.12680767,0.11718296,1.89464102,-0.01371153
,-0.07981451,-0.70497424,-0.0478,-0.05700732,-0.05363342,-0.0657329
,0.08163511,-1.06263979,0.09788937,-0.22008936,1.85898192,0.08773695
,0.06200327,-0.3802791,0.07829525,0.06707749,0.14137152,0.08834448
,-0.07649805,0.78328658,0.12580912,-0.01076061,-0.35937259,-0.13176489
,0.07497022,-0.2331914,-0.11682692,0.04782308,-0.13571422,0.22827948
,-0.23456622,-0.12406075,-0.04466465,0.2311667,-0.12232673,-0.25614032
,-0.36237662,0.11197906,-0.08259534,-0.634934,-0.30822742,-0.93798716
,0.08848668,0.4083417,-0.30910404,0.40950143,0.30815359,0.03266103
,1.03959336,-0.19865537,0.25149713,0.3277561,0.16943092,0.6912597
,0.21721349,-0.30871948,0.88890484,-0.08884043,0.38474549,0.30884107
,-0.40933304,0.30889523,-0.29562966,-0.6271498])
env.unwrapped.set_physics(qpos=goal_qpos, qvel=np.zeros(75))
goal_obs = torch.tensor(env.unwrapped.get_obs()["proprio"].reshape(1,-1), device=model.cfg.device, dtype=torch.float32)
print("goal pose")

z = model.goal_inference(next_obs=goal_obs)

# Create a new environment with RecordVideo wrapper for goal inference example
goal_video_dir = os.path.join(os.getcwd(), "videos_goal_inference")
os.makedirs(goal_video_dir, exist_ok=True)

env_goal, _ = make_humenv(
    num_envs=1,
    unbalanced=unbalanced,
    wrappers=[
        FlattenObservation,
        transform_obs_wrapper,
        RecordEpisodeStatistics,
        lambda env: RecordVideo(env, goal_video_dir, episode_trigger=lambda x: True)
    ],
    state_init="Default",
)

observation, _ = env_goal.reset()
for i in range(3000):
    action = model.act(observation, z, mean=True)
    observation, reward, terminated, truncated, info = env_goal.step(action.cpu().numpy().ravel())
    if terminated or truncated:
        observation, _ = env_goal.reset()

print(f"Goal inference video recording saved to {goal_video_dir}")

if __name__ == "__main__":
    print("Running Meta Motivo Tutorial")
    # Close all environments explicitly to avoid cleanup errors
    env.close()
    if 'env_reward' in locals():
        env_reward.close()
    if 'env_goal' in locals():
        env_goal.close() 